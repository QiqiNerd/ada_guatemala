{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858416a0",
   "metadata": {},
   "source": [
    "## 0: Save intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1953fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HOGARES.DTA with 5 variables.\n",
      "Loaded AUTOIDH.DTA with 3 variables.\n",
      "Loaded ECV02H01.DTA with 6 variables.\n",
      "Loaded CONSUMO5.DTA with 6 variables.\n",
      "Loaded ECV18N15.DTA with 3 variables.\n",
      "Loaded ECV21A16.DTA with 5 variables.\n",
      "Loaded ECV31A16.DTA with 5 variables.\n",
      "Loaded ECV17E14.DTA with 5 variables.\n",
      "Loaded ECV09P05.DTA with 10 variables.\n",
      "Loaded ECV28A16.DTA with 5 variables.\n",
      "Loaded ECV01H01.DTA with 29 variables.\n",
      "Loaded ECV19N15.DTA with 2 variables.\n",
      "Loaded ECOM03.DTA with 6 variables.\n",
      "Loaded ECV11P10.DTA with 7 variables.\n",
      "Loaded ECOM02.DTA with 5 variables.\n"
     ]
    }
   ],
   "source": [
    "field_selection_path = '../metadata/guatemala_variable_filtered_v3.csv'\n",
    "field_df = pd.read_csv(field_selection_path)\n",
    "\n",
    "# included = yes\n",
    "included_df = field_df[field_df['included'].str.lower() == 'yes']\n",
    "\n",
    "module_var_map = defaultdict(list)\n",
    "for _, row in included_df.iterrows():\n",
    "    module_name = row['module'].strip().upper()  # 统一大写并加扩展名\n",
    "    variable_name = row['variable_name'].strip()\n",
    "    module_var_map[module_name].append(variable_name)\n",
    "\n",
    "# Read the corresponding fields of each module\n",
    "dataframes = {} \n",
    "raw_data_path = '../raw_data'\n",
    "\n",
    "for module_file, var_list in module_var_map.items():\n",
    "    module_path = os.path.join(raw_data_path, module_file)\n",
    "    \n",
    "    if not os.path.exists(module_path):\n",
    "        print(f\"Warning: {module_file} not found in raw_data/\")\n",
    "        continue\n",
    "\n",
    "    # Read the required fields in the module\n",
    "    df, meta = pyreadstat.read_dta(module_path, usecols=var_list, apply_value_formats=True, encoding='iso-8859-1')\n",
    "\n",
    "    if 'hogar' in df.columns:\n",
    "        df['hogar'] = df['hogar'].astype(str)\n",
    "        df.rename(columns={'hogar': 'hhid'}, inplace=True)\n",
    "\n",
    "    if 'caso' in df.columns:\n",
    "        df['caso'] = df['caso'].astype(str)\n",
    "    dataframes[module_file] = df\n",
    "    print(f\"Loaded {module_file} with {len(var_list)} variables.\")\n",
    "\n",
    "\n",
    "output_path = '../intermediate_data'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for module_file, df in dataframes.items():\n",
    "    csv_name = module_file.replace('.DTA', '.csv')\n",
    "    df.to_csv(os.path.join(output_path, csv_name), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a668e",
   "metadata": {},
   "source": [
    "## 1: Clean ```HOGARES``` - Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce49a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region    0\n",
      "area      0\n",
      "hhid      0\n",
      "thogar    0\n",
      "factor    0\n",
      "dtype: int64\n",
      "          region    area  hhid  thogar  factor\n",
      "0  metropolitana  urbana   1.0     4.0   541.0\n",
      "1  metropolitana  urbana   2.0     3.0   541.0\n",
      "2  metropolitana  urbana   3.0     6.0   541.0\n",
      "3  metropolitana  urbana   4.0     1.0   541.0\n",
      "4  metropolitana  urbana   5.0     3.0   541.0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: read CSV\n",
    "hogares = pd.read_csv('../intermediate_data/HOGARES.csv', encoding='utf-8-sig')\n",
    "\n",
    "# Step 2: check data\n",
    "missing_report = hogares.isnull().sum()\n",
    "print(missing_report)\n",
    "print(hogares.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc3123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure data type\n",
    "for col in ['hhid', 'thogar']:\n",
    "    if col in hogares.columns:\n",
    "        hogares[col] = hogares[col].astype('Int64').astype(str)\n",
    "\n",
    "hogares = hogares.rename(columns={'factor': 'hh_wgt'})\n",
    "\n",
    "# save csv\n",
    "hogares.to_csv('../cleaned_data/HOGARES_cleaned.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dceadbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7276"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hogares)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407c0c3",
   "metadata": {},
   "source": [
    "## 2. Clean ```AUTOIDH``` - Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09e5e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhid        0\n",
      "idenho_1    0\n",
      "getnicoh    0\n",
      "dtype: int64\n",
      "   hhid     idenho_1      getnicoh\n",
      "0     1    Otro Maya     Indigenas\n",
      "1     2  No indigena  No Indigenas\n",
      "2     3  No indigena  No Indigenas\n",
      "3     4  No indigena  No Indigenas\n",
      "4     5  No indigena  No Indigenas\n"
     ]
    }
   ],
   "source": [
    "autoidh = pd.read_csv('../intermediate_data/AUTOIDH.csv', encoding='utf-8-sig')\n",
    "missing_report = autoidh.isnull().sum()\n",
    "print(missing_report)\n",
    "print(autoidh.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb573e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../cleaned_data/AUTOIDH_cleaned.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nothing needs to be cleaned\n",
    "import shutil\n",
    "shutil.copyfile('../intermediate_data/AUTOIDH.csv', '../cleaned_data/AUTOIDH_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a57957",
   "metadata": {},
   "source": [
    "## 3. Clean ```CONSUMO5``` - Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3d583009",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumo5 = pd.read_csv('../intermediate_data/CONSUMO5.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9117aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depto     0\n",
      "mupio     0\n",
      "sector    0\n",
      "hhid      0\n",
      "upm2      0\n",
      "agreg3    0\n",
      "dtype: int64\n",
      "       depto  mupio  sector  hhid       upm2        agreg3\n",
      "0  guatemala    3.0     3.0   1.0  1031003.0  11687.007812\n",
      "1  guatemala    3.0     3.0   2.0  1031003.0  10821.222656\n",
      "2  guatemala    3.0     3.0   3.0  1031003.0  11324.982422\n",
      "3  guatemala    3.0     3.0   4.0  1031003.0  59760.890625\n",
      "4  guatemala    3.0     3.0   5.0  1031003.0  32664.031250\n"
     ]
    }
   ],
   "source": [
    "missing_report = consumo5.isnull().sum()\n",
    "print(missing_report)\n",
    "print(consumo5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make agreg3 to be per capita per day in 2017 USD PPP\n",
    "conversion_factor = 0.6878405536 \n",
    "consumo5['consumption_per_capita_per_day'] = (consumo5['agreg3'] / 365) * conversion_factor\n",
    "\n",
    "for col in ['mupio', 'sector', 'hhid']:\n",
    "    if col in consumo5.columns:\n",
    "        consumo5[col] = consumo5[col].astype('Int64').astype(str)\n",
    "\n",
    "consumo5.to_csv('../cleaned_data/CONSUMO5_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd04768",
   "metadata": {},
   "source": [
    "## 4. Clean ```ECV01H01``` - Housing characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5287b290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p01a11      47\n",
      "p01a13    2185\n",
      "p01a14    2185\n",
      "p01a35    1975\n",
      "p01a44    3019\n",
      "p01a45    3019\n",
      "dtype: int64\n",
      "   hhid       p01a01 p01a02           p01a03                       p01a04  \\\n",
      "0   1.0  casa formal  block  lamina metalica  ladrillo de cemento o barro   \n",
      "1   2.0  casa formal  adobe  lamina metalica  ladrillo de cemento o barro   \n",
      "2   3.0  casa formal  block         concreto  ladrillo de cemento o barro   \n",
      "3   4.0  casa formal  block         concreto  ladrillo de cemento o barro   \n",
      "4   5.0  casa formal  block         concreto  ladrillo de cemento o barro   \n",
      "\n",
      "  p01a05a p01a05b p01a05c p01a05d p01a05e  ...          p01a14  p01a23  \\\n",
      "0      si      si      si      si      si  ...         publico      no   \n",
      "1      si      si      si      no      si  ...  privado formal      no   \n",
      "2      si      si      si      si      si  ...         publico      no   \n",
      "3      si      si      si      si      si  ...  privado formal      no   \n",
      "4      si      si      si      si      si  ...  privado formal      no   \n",
      "\n",
      "       p01a25                               p01a26  p01a34           p01a35  \\\n",
      "0  la hierven  inodoro conectado a red de drenajes      si  empresa privada   \n",
      "1     ninguno  inodoro conectado a red de drenajes      si  empresa privada   \n",
      "2  la hierven  inodoro conectado a red de drenajes      si  empresa privada   \n",
      "3  la hierven  inodoro conectado a red de drenajes      si  empresa privada   \n",
      "4     ninguno  inodoro conectado a red de drenajes      si  empresa publica   \n",
      "\n",
      "  p01a39 p01a44 p01a45                       p01c01  \n",
      "0     no    NaN    NaN                    alquilada  \n",
      "1     no    NaN    NaN   propia y totalmente pagada  \n",
      "2     no    NaN    NaN            cedida o prestada  \n",
      "3     no    NaN    NaN   propia y totalmente pagada  \n",
      "4     no    NaN    NaN                    alquilada  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "ecv01h01 = pd.read_csv('../intermediate_data/ECV01H01.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv01h01.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv01h01.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with NaN - to be 'missing'\n",
    "categorical_missing_cols = ['p01a11', 'p01a13', 'p01a14', 'p01a35', 'p01a44', 'p01a45']\n",
    "for col in categorical_missing_cols:\n",
    "    if col in ecv01h01.columns:\n",
    "        ecv01h01[col] = ecv01h01[col].fillna('missing')\n",
    "\n",
    "int_str_cols = ['hhid', 'p01a06', 'p01a07', 'p01a08', 'p01a09']\n",
    "for col in int_str_cols:\n",
    "    if col in ecv01h01.columns:\n",
    "        ecv01h01[col] = ecv01h01[col].astype('Int64').astype(str)\n",
    "\n",
    "ecv01h01.to_csv('../cleaned_data/ECV01H01_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1413de6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "   hhid       p01a01 p01a02           p01a03                       p01a04  \\\n",
      "0     1  casa formal  block  lamina metalica  ladrillo de cemento o barro   \n",
      "1     2  casa formal  adobe  lamina metalica  ladrillo de cemento o barro   \n",
      "2     3  casa formal  block         concreto  ladrillo de cemento o barro   \n",
      "3     4  casa formal  block         concreto  ladrillo de cemento o barro   \n",
      "4     5  casa formal  block         concreto  ladrillo de cemento o barro   \n",
      "\n",
      "  p01a05a p01a05b p01a05c p01a05d p01a05e  ...          p01a14  p01a23  \\\n",
      "0      si      si      si      si      si  ...         publico      no   \n",
      "1      si      si      si      no      si  ...  privado formal      no   \n",
      "2      si      si      si      si      si  ...         publico      no   \n",
      "3      si      si      si      si      si  ...  privado formal      no   \n",
      "4      si      si      si      si      si  ...  privado formal      no   \n",
      "\n",
      "       p01a25                               p01a26  p01a34           p01a35  \\\n",
      "0  la hierven  inodoro conectado a red de drenajes      si  empresa privada   \n",
      "1     ninguno  inodoro conectado a red de drenajes      si  empresa privada   \n",
      "2  la hierven  inodoro conectado a red de drenajes      si  empresa privada   \n",
      "3  la hierven  inodoro conectado a red de drenajes      si  empresa privada   \n",
      "4     ninguno  inodoro conectado a red de drenajes      si  empresa publica   \n",
      "\n",
      "  p01a39   p01a44   p01a45                       p01c01  \n",
      "0     no  missing  missing                    alquilada  \n",
      "1     no  missing  missing   propia y totalmente pagada  \n",
      "2     no  missing  missing            cedida o prestada  \n",
      "3     no  missing  missing   propia y totalmente pagada  \n",
      "4     no  missing  missing                    alquilada  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "ecv01h01_clean = pd.read_csv('../cleaned_data/ECV01H01_cleaned.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv01h01_clean.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv01h01_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ed655",
   "metadata": {},
   "source": [
    "## 5. Clean ```ECV02H01``` - Energy Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e07df38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p01b04a    31593\n",
      "p01b04b    31593\n",
      "p01b04c    31593\n",
      "p01b04d    49468\n",
      "dtype: int64\n",
      "   hhid                      item p01b04a p01b04b p01b04c p01b04d\n",
      "0   1.0    candelas y/o veladoras     NaN     NaN     NaN     NaN\n",
      "1   1.0  kerosene (gas corriente)     NaN     NaN     NaN     NaN\n",
      "2   1.0               gas propano      si      no      no     NaN\n",
      "3   1.0                    carbon     NaN     NaN     NaN     NaN\n",
      "4   1.0                  baterias      si      si      no     NaN\n"
     ]
    }
   ],
   "source": [
    "ecv02h01 = pd.read_csv('../intermediate_data/ECV02H01.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv02h01.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv02h01.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548228ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: mapping dict\n",
    "energy_translation = {\n",
    "    \"candelas y/o veladoras\": \"candles\",\n",
    "    \"kerosene (gas corriente)\": \"kerosene\",\n",
    "    \"gas propano\": \"propane_gas\",\n",
    "    \"carbon\": \"charcoal\",\n",
    "    \"baterias\": \"batteries\",\n",
    "    \"electricidad\": \"electricity\",\n",
    "    \"leña o palos\": \"firewood\",\n",
    "    \"otra fuente de energia\": \"other_energy\"\n",
    "}\n",
    "\n",
    "use_map = {\n",
    "    'p01b04a': 'household',\n",
    "    'p01b04b': 'cooking',\n",
    "    'p01b04c': 'home_business',\n",
    "    'p01b04d': 'other'\n",
    "}\n",
    "\n",
    "# Step 3: change missing values\n",
    "for col in use_map.keys():\n",
    "    ecv02h01[col] = ecv02h01[col].fillna('missing')\n",
    "\n",
    "# Step 4: wide to long\n",
    "long_df = pd.melt(\n",
    "    ecv02h01,\n",
    "    id_vars=['hhid', 'item'],\n",
    "    value_vars=list(use_map.keys()),\n",
    "    var_name='use_code',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "# Step 5: generate col names\n",
    "long_df['energy'] = long_df['item'].map(energy_translation)\n",
    "long_df['use'] = long_df['use_code'].map(use_map)\n",
    "long_df['final_col'] = long_df['energy'] + '_' + long_df['use']\n",
    "\n",
    "# Step 6: pivot to wide format\n",
    "ecv02h01_cleaned = long_df.pivot_table(\n",
    "    index='hhid',\n",
    "    columns='final_col',\n",
    "    values='value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Step 7: hhid as string in case errors\n",
    "ecv02h01_cleaned['hhid'] = ecv02h01_cleaned['hhid'].astype('Int64').astype(str)\n",
    "\n",
    "ecv02h01_cleaned.to_csv('../cleaned_data/ECV02H01_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "928a2cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "   hhid batteries_cooking batteries_home_business batteries_household  \\\n",
      "0     1                si                      no                  si   \n",
      "1     2           missing                 missing             missing   \n",
      "2     3                no                      no                  si   \n",
      "3     4           missing                 missing             missing   \n",
      "4     5                no                      no                  si   \n",
      "\n",
      "  batteries_other candles_cooking candles_home_business candles_household  \\\n",
      "0         missing         missing               missing           missing   \n",
      "1         missing         missing               missing           missing   \n",
      "2         missing         missing               missing           missing   \n",
      "3         missing         missing               missing           missing   \n",
      "4         missing              no                    si                no   \n",
      "\n",
      "  candles_other charcoal_cooking  ... kerosene_household kerosene_other  \\\n",
      "0       missing          missing  ...            missing        missing   \n",
      "1       missing          missing  ...            missing        missing   \n",
      "2       missing               no  ...            missing        missing   \n",
      "3       missing          missing  ...            missing        missing   \n",
      "4       missing               no  ...            missing        missing   \n",
      "\n",
      "  other_energy_cooking other_energy_home_business other_energy_household  \\\n",
      "0              missing                    missing                missing   \n",
      "1              missing                    missing                missing   \n",
      "2              missing                    missing                missing   \n",
      "3              missing                    missing                missing   \n",
      "4              missing                    missing                missing   \n",
      "\n",
      "  other_energy_other propane_gas_cooking propane_gas_home_business  \\\n",
      "0            missing                  no                        no   \n",
      "1            missing                  no                        no   \n",
      "2            missing                  no                        no   \n",
      "3            missing                  no                        no   \n",
      "4            missing                  no                        no   \n",
      "\n",
      "  propane_gas_household propane_gas_other  \n",
      "0                    si           missing  \n",
      "1                    si           missing  \n",
      "2                    si           missing  \n",
      "3                    si           missing  \n",
      "4                    si           missing  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "ecv02h01_clean = pd.read_csv('../cleaned_data/ECV02H01_cleaned.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv02h01_clean.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv02h01_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0740f883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7276"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ecv02h01_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9aba0e",
   "metadata": {},
   "source": [
    "## 6. Clean ```ECV09P05``` - Demographic Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa5f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p05a03     11629\n",
      "p05b05      8357\n",
      "p07b01      8244\n",
      "p07b06b    28687\n",
      "p07b29     18000\n",
      "dtype: int64\n",
      "   hhid  caso       sexo  edad     p05a02       p05a03       p05b05  \\\n",
      "0   1.0   1.0   femenino  42.0   jefe (a)    viudo (a)    otro maya   \n",
      "1   1.0   2.0   femenino  23.0   hijo (a)  soltero (a)    otro maya   \n",
      "2   1.0   3.0  masculino  19.0   hijo (a)  soltero (a)    otro maya   \n",
      "3   1.0   4.0   femenino   0.0  nieto (a)          NaN          NaN   \n",
      "4   2.0   1.0  masculino  60.0   jefe (a)    unido (a)  no indigena   \n",
      "\n",
      "          p07b01  p07b06b         p07b29  \n",
      "0  lee y escribe      NaN       primaria  \n",
      "1  lee y escribe      2.0        basicos  \n",
      "2  lee y escribe      6.0  diversificado  \n",
      "3            NaN      NaN            NaN  \n",
      "4  lee y escribe      NaN   preparatoria  \n"
     ]
    }
   ],
   "source": [
    "ecv09p05 = pd.read_csv('../intermediate_data/ECV09P05.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv09p05.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv09p05.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713712d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecv09p05['hhid'] = ecv09p05['hhid'].astype('int64').astype(str)\n",
    "ecv09p05['caso'] = ecv09p05['caso'].astype('int64').astype(str)\n",
    "\n",
    "ecv09p05.columns = ecv09p05.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the total number of people in each family\n",
    "total_members = ecv09p05.groupby('hhid').size().reset_index(name='num_members')\n",
    "\n",
    "# Step 2: Count the number of men and women in each household\n",
    "ecv09p05['sexo'] = ecv09p05['sexo'].str.lower()\n",
    "\n",
    "num_male = ecv09p05[ecv09p05['sexo'] == 'masculino'].groupby('hhid').size().reset_index(name='num_male')\n",
    "num_female = ecv09p05[ecv09p05['sexo'] == 'femenino'].groupby('hhid').size().reset_index(name='num_female')\n",
    "\n",
    "# Step 3: Count children, adults and the elderly (classified by age)\n",
    "# edad=age\n",
    "ecv09p05['edad'] = ecv09p05['edad'].astype(int)\n",
    "\n",
    "children = ecv09p05[ecv09p05['edad'] <= 17].groupby('hhid').size().reset_index(name='num_children')\n",
    "adults = ecv09p05[ecv09p05['edad'] >= 18].groupby('hhid').size().reset_index(name='num_adults')\n",
    "elderly = ecv09p05[ecv09p05['edad'] >= 60].groupby('hhid').size().reset_index(name='num_elderly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb0b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Extract the relevant information of the household head (based on p05a02 = 'jefe (a)')\n",
    "ecv09p05['p05a02'] = ecv09p05['p05a02'].str.lower()\n",
    "ecv09p05['p05a03'] = ecv09p05['p05a03'].str.lower()\n",
    "ecv09p05['p05b05'] = ecv09p05['p05b05'].str.lower()\n",
    "\n",
    "# Take out the row of the household head\n",
    "hh_head_rows = ecv09p05[ecv09p05['p05a02'] == 'jefe (a)'].copy()\n",
    "\n",
    "# Household head information\n",
    "hh_head_info = hh_head_rows[['hhid', 'sexo', 'edad', 'p05a03', 'p05b05', 'p07b01', 'p07b29']].copy()\n",
    "hh_head_info = hh_head_info.rename(columns={\n",
    "    'sexo': 'hh_head_gender',\n",
    "    'edad': 'hh_head_age',\n",
    "    'p05a03': 'hh_head_marital_status',\n",
    "    'p05b05': 'hh_head_ethnicity',\n",
    "    'p07b01': 'hh_head_literacy',\n",
    "    'p07b29': 'hh_head_education_level'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: The person with the highest educational attainment in each household (educational level)\n",
    "# Educational hierarchy ranking mapping\n",
    "edu_rank = {\n",
    "    'preparatoria': 1,\n",
    "    'primaria': 2,\n",
    "    'basicos': 3,\n",
    "    'diversificado': 4,\n",
    "    'universitario': 5,\n",
    "    'post-grado': 6\n",
    "}\n",
    "\n",
    "ecv09p05['p07b29'] = ecv09p05['p07b29'].str.lower()\n",
    "ecv09p05['edu_score'] = ecv09p05['p07b29'].map(edu_rank)\n",
    "\n",
    "max_edu = ecv09p05.groupby('hhid').apply(\n",
    "    lambda x: x.loc[x['edu_score'].idxmax(), 'p07b29'] if x['edu_score'].notna().any() else 'missing'\n",
    ").reset_index(name='max_education_level')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee7b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: The adult female with the highest educational attainment\n",
    "adult_female = ecv09p05[(ecv09p05['sexo'] == 'femenino') & (ecv09p05['edad'] >= 18)].copy()\n",
    "adult_female['edu_score'] = adult_female['p07b29'].map(edu_rank)\n",
    "\n",
    "female_max_edu = adult_female.groupby('hhid').apply(\n",
    "    lambda x: x.loc[x['edu_score'].idxmax(), 'p07b29'] if x['edu_score'].notna().any() else 'missing'\n",
    ").reset_index(name='female_max_education_level')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ef0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: The number of children currently enrolled (edad <= 17 and p07b06b not empty)\n",
    "school_age_children = ecv09p05[(ecv09p05['edad'] <= 17) & (ecv09p05['p07b06b'].notna())]\n",
    "num_enrolled_children = school_age_children.groupby('hhid').size().reset_index(name='num_children_enrolled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init household dataframe\n",
    "from functools import reduce\n",
    "\n",
    "dfs = [\n",
    "    total_members,\n",
    "    num_male,\n",
    "    num_female,\n",
    "    children,\n",
    "    adults,\n",
    "    elderly,\n",
    "    hh_head_info,\n",
    "    max_edu,\n",
    "    female_max_edu,\n",
    "    num_enrolled_children\n",
    "]\n",
    "\n",
    "# Use reduce to merge all Dataframes (left join)\n",
    "from functools import reduce\n",
    "household_summary = reduce(lambda left, right: pd.merge(left, right, on='hhid', how='left'), dfs)\n",
    "\n",
    "# Completion of missing values\n",
    "household_summary['hh_head_education_level'] = household_summary['hh_head_education_level'].fillna('missing')\n",
    "household_summary['female_max_education_level'] = household_summary['female_max_education_level'].fillna('missing')\n",
    "\n",
    "count_vars = [\n",
    "    'num_male', 'num_female', 'num_children', 'num_adults',\n",
    "    'num_elderly', 'num_children_enrolled'\n",
    "]\n",
    "\n",
    "for col in count_vars:\n",
    "    if col in household_summary.columns:\n",
    "        household_summary[col] = household_summary[col].fillna(0).astype(int)\n",
    "\n",
    "# Sort hhid by integer\n",
    "household_summary['hhid_sort'] = household_summary['hhid'].astype(int)\n",
    "household_summary = household_summary.sort_values('hhid_sort').drop(columns='hhid_sort')\n",
    "\n",
    "household_summary['hhid'] = household_summary['hhid'].astype(str)\n",
    "household_summary['hh_head_age'] = household_summary['hh_head_age'].astype(int).astype(str)\n",
    "\n",
    "household_summary.to_csv('../cleaned_data/ECV09P05_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "50fee500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "   hhid  num_members  num_male  num_female  num_children  num_adults  \\\n",
      "0     1            4         1           3             1           3   \n",
      "1     2            3         2           1             0           3   \n",
      "2     3            6         5           1             2           4   \n",
      "3     4            1         0           1             0           1   \n",
      "4     5            3         2           1             1           2   \n",
      "\n",
      "   num_elderly hh_head_gender  hh_head_age hh_head_marital_status  \\\n",
      "0            0       femenino           42              viudo (a)   \n",
      "1            2      masculino           60              unido (a)   \n",
      "2            0      masculino           51             casado (a)   \n",
      "3            1       femenino           89              viudo (a)   \n",
      "4            0      masculino           49             casado (a)   \n",
      "\n",
      "  hh_head_ethnicity hh_head_literacy hh_head_education_level  \\\n",
      "0         otro maya    lee y escribe                primaria   \n",
      "1       no indigena    lee y escribe            preparatoria   \n",
      "2       no indigena    lee y escribe            preparatoria   \n",
      "3       no indigena    lee y escribe            preparatoria   \n",
      "4       no indigena    lee y escribe           universitario   \n",
      "\n",
      "  max_education_level female_max_education_level  num_children_enrolled  \n",
      "0       diversificado                    basicos                      0  \n",
      "1        preparatoria               preparatoria                      0  \n",
      "2       diversificado                    missing                      2  \n",
      "3        preparatoria               preparatoria                      0  \n",
      "4       universitario              diversificado                      0  \n"
     ]
    }
   ],
   "source": [
    "ecv09p05_cleaned = pd.read_csv('../cleaned_data/ECV09P05_cleaned.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv09p05_cleaned.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv09p05_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350dda92",
   "metadata": {},
   "source": [
    "## 7. Clean ```ECV11P10``` - Household's living activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f23fb44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p10d01     15024\n",
      "p10d03     27023\n",
      "p10d04     27020\n",
      "p10d06     27023\n",
      "p10e05a    31482\n",
      "dtype: int64\n",
      "   hhid  caso p10d01 p10d03 p10d04 p10d06 p10e05a\n",
      "0   1.0   1.0    NaN    NaN    NaN    NaN     NaN\n",
      "1   1.0   2.0     no    NaN    NaN    NaN     NaN\n",
      "2   1.0   3.0     no    NaN    NaN    NaN     NaN\n",
      "3   2.0   1.0     no    NaN    NaN    NaN     NaN\n",
      "4   2.0   2.0    NaN    NaN    NaN    NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "ecv11p10 = pd.read_csv('../intermediate_data/ECV11P10.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv11p10.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv11p10.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0927d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type\n",
    "ecv11p10.columns = ecv11p10.columns.str.lower()\n",
    "ecv11p10['hhid'] = ecv11p10['hhid'].astype('int64').astype(str)\n",
    "ecv11p10['caso'] = ecv11p10['caso'].astype('int64').astype(str)\n",
    "# Only retain the household head information (caso == '1')\n",
    "hh_head_livelihood = ecv11p10[ecv11p10['caso'] == '1'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ef524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the specified field\n",
    "selected_cols = ['hhid', 'p10d01', 'p10d03', 'p10d04', 'p10d06', 'p10e05a']\n",
    "hh_head_livelihood = hh_head_livelihood[selected_cols]\n",
    "\n",
    "# Handle missing values (fill 'missing' in all columns except hhid)\n",
    "for col in hh_head_livelihood.columns:\n",
    "    if col != 'hhid':\n",
    "        hh_head_livelihood[col] = hh_head_livelihood[col].fillna('missing')\n",
    "\n",
    "hh_head_livelihood.to_csv('../cleaned_data/ECV11P10_cleaned.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75d22270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "   hhid   p10d01   p10d03   p10d04   p10d06  p10e05a\n",
      "0     1  missing  missing  missing  missing  missing\n",
      "1     2       no  missing  missing  missing  missing\n",
      "2     3       no  missing  missing  missing  missing\n",
      "3     4  missing  missing  missing  missing  missing\n",
      "4     5       no  missing  missing  missing  missing\n"
     ]
    }
   ],
   "source": [
    "ecv11p10_clean = pd.read_csv('../cleaned_data/ECV11P10_cleaned.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv11p10_clean.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv11p10_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1be3af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hhids = pd.DataFrame({'hhid': [i for i in range(1, 7277)]})\n",
    "\n",
    "df_full = full_hhids.merge(ecv11p10_clean, on='hhid', how='left')\n",
    "\n",
    "# except for hhid，all missing value = 'missing'\n",
    "for col in df_full.columns:\n",
    "    if col != 'hhid':\n",
    "        df_full[col] = df_full[col].fillna('missing')\n",
    "\n",
    "df_full.to_csv('../cleaned_data/ECV11P10_cleaned.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5934869f",
   "metadata": {},
   "source": [
    "## 8. Clean ```ECV17E14``` - Home Equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50978bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p10d01     15024\n",
      "p10d03     27023\n",
      "p10d04     27020\n",
      "p10d06     27023\n",
      "p10e05a    31482\n",
      "dtype: int64\n",
      "   hhid                  tipo                       item p14a01  p14a02\n",
      "0   1.0   articulos de cocina  estufa de gas / electrica     si     2.0\n",
      "1   1.0   articulos de cocina        horno de microondas     no     NaN\n",
      "2   1.0   articulos de cocina         horno convencional     no     NaN\n",
      "3   1.0   articulos de cocina              refrigeradora     si     1.0\n",
      "4   1.0   articulos de cocina         cafetera electrica     no     NaN\n"
     ]
    }
   ],
   "source": [
    "ecv17e14 = pd.read_csv('../intermediate_data//ECV17E14.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv11p10.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv17e14.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb0331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 ['estufa de gas / electrica' 'horno de microondas' 'horno convencional'\n",
      " 'refrigeradora' 'cafetera electrica' 'licuadora' 'exprimidor de jugos'\n",
      " 'molino de nixtamal y otros' 'computadora personal' 'impresora'\n",
      " 'camara fotografica' 'radio transistor' 'componente con cd'\n",
      " 'grabadora / radiograbadora' 'maquina de escribir' 'televisor'\n",
      " 'camara de video' 'video - camara / cassetera' 'otro de esparcimiento'\n",
      " 'beeper' 'telefono' 'plancha electrica' 'plancha de brazas' 'lavadora'\n",
      " 'secadora' 'ventilador' 'aspiradora' 'maquina de coser'\n",
      " 'otro articulo del hogar' 'automovil' 'pick up' 'camionetilla'\n",
      " 'moto o motoneta' 'bicicleta' 'carreta de bueyes' 'camion'\n",
      " 'bote o lancha' 'otro vehiculo']\n"
     ]
    }
   ],
   "source": [
    "# Check the number of non-repetitive values in the \"item\" column\n",
    "num_unique, unique_values = ecv17e14[\"item\"].nunique(), ecv17e14[\"item\"].unique()\n",
    "print(num_unique, unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2872211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# standardized column names are in lowercase\n",
    "ecv17e14.columns = ecv17e14.columns.str.lower()\n",
    "ecv17e14['hhid'] = ecv17e14['hhid'].astype('Int64')\n",
    "\n",
    "# Handle the outlier p14a01 and generate the \"Quantity of Items\" column\n",
    "ecv17e14['p14a01'] = ecv17e14['p14a01'].astype(str).str.lower()\n",
    "\n",
    "def clean_quantity(row):\n",
    "    if row['p14a01'] == 'no':\n",
    "        return 0\n",
    "    elif row['p14a01'] == 'si':\n",
    "        return row['p14a02']\n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "ecv17e14['p14a02'] = ecv17e14.apply(clean_quantity, axis=1)\n",
    "ecv17e14['p14a02'] = ecv17e14['p14a02'].fillna(0).astype(int)\n",
    "\n",
    "# Use OrderedDict to retain the translation order of items\n",
    "item_translation = OrderedDict([\n",
    "    ('estufa de gas / electrica', 'gas_stove'),\n",
    "    ('horno de microondas', 'microwave'),\n",
    "    ('horno convencional', 'oven'),\n",
    "    ('refrigeradora', 'refrigerator'),\n",
    "    ('cafetera electrica', 'coffee_maker'),\n",
    "    ('licuadora', 'blender'),\n",
    "    ('exprimidor de jugos', 'juicer'),\n",
    "    ('molino de nixtamal y otros', 'nixtamal_mill'),\n",
    "    ('computadora personal', 'computer'),\n",
    "    ('impresora', 'printer'),\n",
    "    ('camara fotografica', 'camera'),\n",
    "    ('radio transistor', 'radio'),\n",
    "    ('componente con cd', 'cd_player'),\n",
    "    ('grabadora / radiograbadora', 'tape_recorder'),\n",
    "    ('maquina de escribir', 'typewriter'),\n",
    "    ('televisor', 'tv'),\n",
    "    ('camara de video', 'video_camera'),\n",
    "    ('video - camara / cassetera', 'cassette_camera'),\n",
    "    ('otro de esparcimiento', 'other_entertainment'),\n",
    "    ('beeper', 'beeper'),\n",
    "    ('telefono', 'phone'),\n",
    "    ('plancha electrica', 'electric_iron'),\n",
    "    ('plancha de brazas', 'charcoal_iron'),\n",
    "    ('lavadora', 'washer'),\n",
    "    ('secadora', 'dryer'),\n",
    "    ('ventilador', 'fan'),\n",
    "    ('aspiradora', 'vacuum'),\n",
    "    ('maquina de coser', 'sewing_machine'),\n",
    "    ('otro articulo del hogar', 'other_household_item'),\n",
    "    ('automovil', 'car'),\n",
    "    ('pick up', 'pickup'),\n",
    "    ('camionetilla', 'small_truck'),\n",
    "    ('moto o motoneta', 'motorcycle'),\n",
    "    ('bicicleta', 'bicycle'),\n",
    "    ('carreta de bueyes', 'ox_cart'),\n",
    "    ('camion', 'truck'),\n",
    "    ('bote o lancha', 'boat'),\n",
    "    ('otro vehiculo', 'other_vehicle')\n",
    "])\n",
    "\n",
    "# Add the translation column and the variable list\n",
    "ecv17e14['item_en'] = ecv17e14['item'].map(item_translation)\n",
    "ecv17e14['var_name'] = 'num_' + ecv17e14['item_en']\n",
    "\n",
    "# pivot to wide format\n",
    "ecv17e14_cleaned = ecv17e14.pivot_table(\n",
    "    index='hhid',\n",
    "    columns='var_name',\n",
    "    values='p14a02',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "ecv17e14_cleaned['hhid_sort'] = ecv17e14_cleaned['hhid'].astype(int)\n",
    "ecv17e14_cleaned = ecv17e14_cleaned.sort_values('hhid_sort').drop(columns='hhid_sort')\n",
    "\n",
    "# Arrange the variable columns in the original order\n",
    "ordered_cols = ['hhid'] + ['num_' + item_translation[item] for item in item_translation]\n",
    "ecv17e14_cleaned = ecv17e14_cleaned[ordered_cols]\n",
    "\n",
    "ecv17e14_cleaned.to_csv('../cleaned_data/ECV17E14_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fb67e",
   "metadata": {},
   "source": [
    "## 9. Clean ```ECV18N15``` - Enterprise Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9542fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p15b02    4624\n",
      "dtype: int64\n",
      "   hhid p15b01  p15b02\n",
      "0   1.0     no     NaN\n",
      "1   2.0     si     1.0\n",
      "2   3.0     no     NaN\n",
      "3   4.0     no     NaN\n",
      "4   5.0     si     1.0\n"
     ]
    }
   ],
   "source": [
    "ecv18n15 = pd.read_csv('../intermediate_data//ECV18N15.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv18n15.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv18n15.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de01339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        hhid p15b01  p15b02\n",
      "1446  1447.0     no     1.0\n",
      "2239  2240.0     no     1.0\n"
     ]
    }
   ],
   "source": [
    "ecv18n15[\"p15b01\"] = ecv18n15[\"p15b01\"].str.lower()\n",
    "inconsistent = ecv18n15[(ecv18n15[\"p15b01\"] == \"no\") & (ecv18n15[\"p15b02\"] > 0)]\n",
    "print(inconsistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecv18n15['hhid'] = ecv18n15['hhid'].astype('Int64').astype(str)\n",
    "ecv18n15['p15b02'] = ecv18n15['p15b02'].fillna(0).astype(int).astype(str)\n",
    "ecv18n15.to_csv('../cleaned_data/ECV18N15_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b556d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "   hhid p15b01  p15b02\n",
      "0     1     no       0\n",
      "1     2     si       1\n",
      "2     3     no       0\n",
      "3     4     no       0\n",
      "4     5     si       1\n"
     ]
    }
   ],
   "source": [
    "ecv18n15_clean = pd.read_csv('../cleaned_data/ECV18N15_cleaned.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv18n15_clean.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv18n15_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd4c538",
   "metadata": {},
   "source": [
    "## 10. Check ```ECV19N15``` - Business details - whether remain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8e0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "   hhid                    p15b04\n",
      "0   2.0   industria manufacturera\n",
      "1   5.0                  comercio\n",
      "2   8.0                  comercio\n",
      "3   9.0   industria manufacturera\n",
      "4   9.0   industria manufacturera\n"
     ]
    }
   ],
   "source": [
    "ecv19n15 = pd.read_csv('../intermediate_data//ECV19N15.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv19n15.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv19n15.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba66ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 [' industria manufacturera' ' comercio' ' servicios financieros'\n",
      " ' servicios de salud, sociales y personales' ' enseÑanza' ' construccion'\n",
      " ' agricultura, ganaderia, caza y pesca'\n",
      " ' transporte, almacenamiento y comunicaciones'\n",
      " ' administracion publica y defensa' ' actividad no especificada'\n",
      " ' electricidad, gas y agua' ' explotacion de minas y canteras']\n"
     ]
    }
   ],
   "source": [
    "# Check the number of non-repetitive values in the \"item\" column\n",
    "num_unique, unique_values = ecv19n15[\"p15b04\"].nunique(), ecv19n15[\"p15b04\"].unique()\n",
    "print(num_unique, unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6070db0",
   "metadata": {},
   "source": [
    "Veriry first: For each hhid, is the p15b02 value (number of enterprises) in ecv18n15_cleaned equal to the number of rows (corresponding number of enterprise industries) of the hhid in ecv19n15?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95298466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ecv18n15 = pd.read_csv('../cleaned_data/ECV18N15_cleaned.csv', encoding='utf-8-sig')\n",
    "ecv19n15 = pd.read_csv('../intermediate_data/ECV19N15.csv', encoding='utf-8-sig')\n",
    "\n",
    "ecv18n15['hhid'] = ecv18n15['hhid'].astype(int)\n",
    "ecv18n15['p15b02'] = ecv18n15['p15b02'].astype(int)\n",
    "ecv19n15.columns = ecv19n15.columns.str.lower()\n",
    "ecv19n15['hhid'] = ecv19n15['hhid'].astype(int)\n",
    "\n",
    "# Only the families with business in ecv18n15 (p15b02 > 0) will be retained.\n",
    "business_hh = ecv18n15[ecv18n15['p15b02'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ 有 10 个家庭存在不一致。以下为示例：\n",
      "      hhid  p15b02  num_industries_reported\n",
      "134    336       2                        1\n",
      "140    346       2                        1\n",
      "141    347       2                        1\n",
      "549   1306       1                        0\n",
      "566   1349       2                        1\n",
      "1085  2763       2                        0\n",
      "1086  2764       2                        0\n",
      "1244  3120       3                        2\n",
      "1726  4599       2                        1\n",
      "2121  5641       1                        0\n"
     ]
    }
   ],
   "source": [
    "# Count the number of industry items in ecv19n15\n",
    "industry_counts = ecv19n15.groupby('hhid').size().reset_index(name='num_industries_reported')\n",
    "# merge\n",
    "validation_df = business_hh.merge(industry_counts, on='hhid', how='left')\n",
    "validation_df['num_industries_reported'] = validation_df['num_industries_reported'].fillna(0).astype(int)\n",
    "\n",
    "# Verify consistency\n",
    "validation_df['match'] = validation_df['p15b02'] == validation_df['num_industries_reported']\n",
    "mismatched = validation_df[validation_df['match'] == False]\n",
    "\n",
    "if mismatched.empty:\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(f\"❗ 有 {len(mismatched)} 个家庭存在不一致。以下为示例：\") \n",
    "    print(mismatched[['hhid', 'p15b02', 'num_industries_reported']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edaa2f5",
   "metadata": {},
   "source": [
    "## 11. Clean ```ECV21A16``` - Amount of land owned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "    hhid                         tipo                            tcuerda  \\\n",
      "0   28.0                     agricola  cuerda/tarea de 25 varas por lado   \n",
      "1   31.0                     agricola  cuerda/tarea de 25 varas por lado   \n",
      "2  297.0                     agricola  cuerda/tarea de 25 varas por lado   \n",
      "3  363.0  mixta (agricola y pecuaria)  cuerda/tarea de 25 varas por lado   \n",
      "4  378.0                     agricola  cuerda/tarea de 25 varas por lado   \n",
      "\n",
      "   p16a07a  p16a07b  \n",
      "0      1.0  manzana  \n",
      "1     10.0   cuerda  \n",
      "2     10.0  manzana  \n",
      "3      2.0   cuerda  \n",
      "4      5.0  manzana  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ecv21a16 = pd.read_csv('../intermediate_data/ECV21A16.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv21a16.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv21a16.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9ec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: hhid\n",
      "Unique values (2523): [ 28.  31. 297. 363. 378. 471. 472. 477. 478. 508.]\n",
      "\n",
      "Column: tipo\n",
      "Unique values (2): ['agricola' 'mixta (agricola y pecuaria)']\n",
      "\n",
      "Column: tcuerda\n",
      "Unique values (8): ['cuerda/tarea de 25 varas por lado' 'cuerda/tarea de 20 varas por lado'\n",
      " 'cuerda/tarea de 30 varas por lado' 'cuerda/tarea de 40 varas por lado'\n",
      " 'cuerda/tarea de 32 varas por lado' 'cuerda/tarea de 24 varas por lado'\n",
      " 'cuerda/tarea de 28 varas por lado' 'cuerda/tarea de 50 varas por lado']\n",
      "\n",
      "Column: p16a07a\n",
      "Unique values (109): [ 1.  10.   2.   5.   3.  12.   1.5  7.   4.5  4. ]\n",
      "\n",
      "Column: p16a07b\n",
      "Unique values (5): ['manzana' 'cuerda' 'hectarea' 'caballeria' 'tarea']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in ecv21a16.columns:\n",
    "    unique_vals = ecv21a16[col].dropna().unique()\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Unique values ({len(unique_vals)}): {unique_vals[:10]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ecv21a16.columns = ecv21a16.columns.str.lower()\n",
    "ecv21a16['hhid'] = ecv21a16['hhid'].astype(int)\n",
    "\n",
    "# Extract the vara value (\"xx varas\" in tcuerda)\n",
    "def extract_vara(val):\n",
    "    match = re.search(r'(\\d+)', val)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "ecv21a16['vara_length'] = ecv21a16['tcuerda'].apply(extract_vara)\n",
    "\n",
    "# Define the unit conversion function (return square meters)\n",
    "def convert_to_sqm(row):\n",
    "    value = row['p16a07a']\n",
    "    unit = str(row['p16a07b']).strip().lower()\n",
    "    vara = row['vara_length']\n",
    "    \n",
    "    if pd.isna(value) or value == '':\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        value = float(value)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    if unit == 'manzana':\n",
    "        return value * 6987\n",
    "    elif unit == 'hectarea':\n",
    "        return value * 10000\n",
    "    elif unit == 'caballeria':\n",
    "        return value * 78.58 * 10000\n",
    "    elif unit in ['cuerda', 'tarea']:\n",
    "        if pd.isna(vara):\n",
    "            return 0\n",
    "        else:\n",
    "            vara_length_m = vara * 0.8421\n",
    "            return value * (vara_length_m ** 2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply conversion functions\n",
    "ecv21a16['land_area_sqm'] = ecv21a16.apply(convert_to_sqm, axis=1)\n",
    "\n",
    "# Apply conversion functions\n",
    "land_area_by_hh = ecv21a16.groupby('hhid')['land_area_sqm'].sum().reset_index()\n",
    "\n",
    "full_hhids = pd.DataFrame({'hhid': [i for i in range(1, 7277)]})\n",
    "\n",
    "# Summarize the existing area according to hhid\n",
    "land_area_by_hh = ecv21a16.groupby('hhid')['land_area_sqm'].sum().reset_index()\n",
    "land_area_by_hh = full_hhids.merge(land_area_by_hh, on='hhid', how='left')\n",
    "land_area_by_hh['land_area_sqm'] = land_area_by_hh['land_area_sqm'].fillna(0)\n",
    "\n",
    "land_area_by_hh.to_csv('../cleaned_data/ECV21A16_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ef35e",
   "metadata": {},
   "source": [
    "## 12. Clean ```ECV28A16``` - Agricultural equipment and facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7901400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p16g02    92052\n",
      "dtype: int64\n",
      "   hhid      tipo                        item p16g01  p16g02\n",
      "0  28.0  agricola  implementos de tiro animal     no     NaN\n",
      "1  28.0  agricola                     tractor     no     NaN\n",
      "2  28.0  agricola    implementos para tractor     no     NaN\n",
      "3  28.0  agricola                 cosechadora     no     NaN\n",
      "4  28.0  agricola    sembradora o cultivadora     no     NaN\n"
     ]
    }
   ],
   "source": [
    "ecv28a16 = pd.read_csv('../intermediate_data/ECV28A16.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv28a16.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv28a16.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b03152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 ['implementos de tiro animal' 'tractor' 'implementos para tractor'\n",
      " 'cosechadora' 'sembradora o cultivadora' 'bomba de agua'\n",
      " 'camion, camioneta, jeep' 'animales de trabajo' 'bomba fumigadora'\n",
      " 'planta electrica' 'equipo de riego' 'pequeÑas herramientas' 'ordeÑadora'\n",
      " 'picadora de zacate' 'desgranadora' 'carretas' 'otro equipo'\n",
      " 'cobertizo/galera' 'molinos' 'tanques' 'pozos' 'baÑaderos' 'silos'\n",
      " 'secaderos' 'abrevaderos' 'otra instalacion']\n",
      "['no' 'si']\n",
      "[ nan   1.  24.   5.   2.   6.   4.   3.  10.   8.  11.   7.  12.   9.\n",
      "  17.  14.  15.  30.  18.  20.  33.  13.  22.  16.   0. 110. 100.  80.\n",
      "  19.  25.  21.  44.  60. 200.  63. 101.  32.  23. 126.  40.  28.  70.\n",
      "  65.  29.  59.  48.  54.  58.  51. 132. 145. 112.]\n"
     ]
    }
   ],
   "source": [
    "print(ecv28a16['item'].nunique(), ecv28a16['item'].unique())\n",
    "print(ecv28a16['p16g01'].unique())\n",
    "print(ecv28a16['p16g02'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cdd3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Filter out the records where p16g01 is \"no\"\n",
    "p16g01 = ecv28a16[ecv28a16[\"p16g01\"].str.lower() == \"no\"]\n",
    "# Check whether the p16g02 of all these records is 0\n",
    "all_na = p16g01[\"p16g02\"].isna().all()\n",
    "print(all_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfda82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "ecv28a16 = pd.read_csv('../intermediate_data/ECV28A16.csv', encoding='utf-8-sig')\n",
    "ecv28a16.columns = ecv28a16.columns.str.lower()\n",
    "\n",
    "ecv28a16['hhid'] = ecv28a16['hhid'].astype('Int64').astype(str)\n",
    "\n",
    "# When cleaning p16g01 = 'no', p16g02 should be set to 0\n",
    "ecv28a16['p16g01'] = ecv28a16['p16g01'].str.lower().str.strip()\n",
    "ecv28a16['p16g02'] = ecv28a16.apply(\n",
    "    lambda row: 0 if row['p16g01'] == 'no' else row['p16g02'],\n",
    "    axis=1\n",
    ")\n",
    "ecv28a16['p16g02'] = ecv28a16['p16g02'].fillna(0).astype(int)\n",
    "\n",
    "# Use OrderedDict to maintain the translation order of the facility\n",
    "item_translation = OrderedDict([\n",
    "    ('implementos de tiro animal', 'animal_drawn_equipment'),\n",
    "    ('tractor', 'tractor'),\n",
    "    ('implementos para tractor', 'tractor_equipment'),\n",
    "    ('cosechadora', 'harvester'),\n",
    "    ('sembradora o cultivadora', 'seeder_cultivator'),\n",
    "    ('bomba de agua', 'water_pump'),\n",
    "    ('camion, camioneta, jeep', 'vehicle'),\n",
    "    ('animales de trabajo', 'work_animals'),\n",
    "    ('bomba fumigadora', 'fumigation_pump'),\n",
    "    ('planta electrica', 'generator'),\n",
    "    ('equipo de riego', 'irrigation_equipment'),\n",
    "    ('pequeÑas herramientas', 'small_tools'),\n",
    "    ('ordeÑadora', 'milking_machine'),\n",
    "    ('picadora de zacate', 'forage_chopper'),\n",
    "    ('desgranadora', 'sheller'),\n",
    "    ('carretas', 'carts'),\n",
    "    ('otro equipo', 'other_equipment'),\n",
    "    ('cobertizo/galera', 'shed'),\n",
    "    ('molinos', 'mills'),\n",
    "    ('tanques', 'tanks'),\n",
    "    ('pozos', 'wells'),\n",
    "    ('baÑaderos', 'baths'),\n",
    "    ('silos', 'silos'),\n",
    "    ('secaderos', 'dryers'),\n",
    "    ('abrevaderos', 'watering_troughs'),\n",
    "    ('otra instalacion', 'other_facility')\n",
    "])\n",
    "\n",
    "# Add English variable names\n",
    "ecv28a16['item_en'] = ecv28a16['item'].map(item_translation)\n",
    "ecv28a16['var_name'] = 'num_' + ecv28a16['item_en']\n",
    "\n",
    "# pivot and aggregate into sum (for handling duplicate rows)\n",
    "wide_df = ecv28a16.pivot_table(\n",
    "    index='hhid',\n",
    "    columns='var_name',\n",
    "    values='p16g02',\n",
    "    aggfunc='sum'\n",
    ").reset_index()\n",
    "\n",
    "# Construct the complete hhid (1-7276), and the completion is 0\n",
    "full_hhids = pd.DataFrame({'hhid': [str(i) for i in range(1, 7277)]})\n",
    "wide_df = full_hhids.merge(wide_df, on='hhid', how='left')\n",
    "wide_df = wide_df.fillna(0).copy()\n",
    "\n",
    "# set original orger\n",
    "ordered_cols = ['hhid'] + ['num_' + item_translation[item] for item in item_translation]\n",
    "wide_df = wide_df[ordered_cols]\n",
    "\n",
    "wide_df['hhid_sort'] = wide_df['hhid'].astype(int)\n",
    "wide_df = wide_df.sort_values('hhid_sort').drop(columns='hhid_sort')\n",
    "wide_df['hhid'] = wide_df['hhid'].astype(str)\n",
    "\n",
    "wide_df.to_csv('../cleaned_data/ECV28A16_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43444b7",
   "metadata": {},
   "source": [
    "## 13. Clean ```ECV31A16``` - Livestock Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10accf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "   hhid      tipo                    item p16j02  p16j03\n",
      "0  38.0  pecuaria  vacas, toros, terneros     no     0.0\n",
      "1  38.0  pecuaria                  cabras     no     0.0\n",
      "2  38.0  pecuaria                  ovejas     no     0.0\n",
      "3  38.0  pecuaria                  cerdos     no     0.0\n",
      "4  38.0  pecuaria                 conejos     no     0.0\n"
     ]
    }
   ],
   "source": [
    "ecv31a16 = pd.read_csv('../intermediate_data/ECV31A16.csv', encoding='utf-8-sig')\n",
    "missing_report = ecv31a16.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecv31a16.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27847d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 ['vacas, toros, terneros' 'cabras' 'ovejas' 'cerdos' 'conejos'\n",
      " 'gallinas y pollos' 'pavos o chompipes' 'patos' 'caballos, burros, mulas'\n",
      " 'colmenas' 'otros animales']\n",
      "['no' 'si']\n",
      "[0.000e+00 1.100e+01 1.000e+00 2.000e+00 7.000e+00 1.000e+01 4.000e+00\n",
      " 8.000e+00 1.400e+01 1.600e+01 3.000e+00 1.200e+01 6.000e+00 4.500e+01\n",
      " 9.250e+02 5.000e+00 2.000e+01 1.300e+01 1.500e+01 2.500e+01 3.600e+01\n",
      " 3.500e+01 2.300e+01 1.700e+01 9.000e+00 5.000e+02 3.700e+01 7.500e+01\n",
      " 1.800e+01 2.600e+01 4.000e+01 3.000e+01 6.500e+01 2.400e+01 6.000e+01\n",
      " 2.200e+01 2.800e+01 2.700e+01 8.000e+01 4.800e+01 1.000e+03 1.800e+02\n",
      " 5.000e+01 1.000e+02 3.000e+03 2.100e+01 3.500e+02 1.900e+01 3.400e+01\n",
      " 3.800e+01 3.100e+01 2.900e+01 3.300e+01 7.000e+01 1.750e+02 6.700e+01\n",
      " 5.500e+01 3.200e+01 2.050e+02 1.600e+02 1.300e+02 1.200e+02 2.300e+04\n",
      " 2.000e+02 6.100e+01 2.500e+02 4.000e+02 4.400e+01 4.300e+01 1.200e+03\n",
      " 5.200e+01 1.500e+02 6.000e+02 4.000e+03 1.500e+03 8.000e+02 4.600e+01\n",
      " 4.100e+01 1.224e+04 9.000e+01 5.900e+01 8.300e+01 8.500e+01 5.300e+01\n",
      " 6.400e+01 2.250e+02 4.700e+01 1.215e+04 7.200e+01 1.020e+02 4.200e+01\n",
      " 8.900e+01]\n"
     ]
    }
   ],
   "source": [
    "print(ecv31a16['item'].nunique(), ecv31a16['item'].unique())\n",
    "print(ecv31a16['p16j02'].unique())\n",
    "print(ecv31a16['p16j03'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70568050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "ecv31a16.columns = ecv31a16.columns.str.lower()\n",
    "\n",
    "ecv31a16['hhid'] = ecv31a16['hhid'].astype(pd.Int64Dtype()).astype(str)\n",
    "\n",
    "# Use OrderedDict to maintain the translation order of animals\n",
    "animal_translation = OrderedDict([\n",
    "    ('vacas, toros, terneros', 'cattle'),\n",
    "    ('cabras', 'goats'),\n",
    "    ('ovejas', 'sheep'),\n",
    "    ('cerdos', 'pigs'),\n",
    "    ('conejos', 'rabbits'),\n",
    "    ('gallinas y pollos', 'chickens'),\n",
    "    ('pavos o chompipes', 'turkeys'),\n",
    "    ('patos', 'ducks'),\n",
    "    ('caballos, burros, mulas', 'horses_donkeys_mules'),\n",
    "    ('colmenas', 'beehives'),\n",
    "    ('otros animales', 'other_animals')\n",
    "])\n",
    "\n",
    "# 名Add English variable names\n",
    "ecv31a16['item_en'] = ecv31a16['item'].map(animal_translation)\n",
    "ecv31a16['var_name'] = 'num_' + ecv31a16['item_en']\n",
    "\n",
    "# pivot to wide format\n",
    "wide_df = ecv31a16.pivot_table(\n",
    "    index='hhid',\n",
    "    columns='var_name',\n",
    "    values='p16j03',\n",
    "    aggfunc='sum'\n",
    ").reset_index()\n",
    "\n",
    "# The complete hhid (1-7276) was constructed, and the completion was 0\n",
    "full_hhids = pd.DataFrame({'hhid': [str(i) for i in range(1, 7277)]})\n",
    "wide_df = full_hhids.merge(wide_df, on='hhid', how='left')\n",
    "wide_df = wide_df.fillna(0).copy()\n",
    "\n",
    "# clarify the column sequence (hhid 11 columns of animals)\n",
    "ordered_cols = ['hhid'] + ['num_' + animal_translation[item] for item in animal_translation]\n",
    "wide_df = wide_df[ordered_cols]\n",
    "\n",
    "wide_df['hhid_sort'] = wide_df['hhid'].astype(int)\n",
    "wide_df = wide_df.sort_values('hhid_sort').drop(columns='hhid_sort')\n",
    "wide_df['hhid'] = wide_df['hhid'].astype(str)\n",
    "\n",
    "wide_df.to_csv('../cleaned_data/ECV31A16_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a4a44",
   "metadata": {},
   "source": [
    "## 14. Clean ```ECOM02``` - road availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f98116b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n",
      "   depto  mupio  sector    c02act c02a01\n",
      "0      1      5      50  carreter     si\n",
      "1      1      5      50  carreter     no\n",
      "2      1      5      50  camino d     no\n",
      "3      1      5      50   veredas     no\n",
      "4      1      5      50  mar, lag     no\n"
     ]
    }
   ],
   "source": [
    "ecom02 = pd.read_csv('../intermediate_data/ECOM02.csv', encoding='utf-8-sig')\n",
    "missing_report = ecom02.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecom02.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5dea3a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ['guatemala' 'el progreso' 'sacatepequez' 'chimaltenango' 'escuintla'\n",
      " 'santa rosa' 'solola' 'totonicapan' 'quetzaltenango' 'suchitepequez'\n",
      " 'retalhuleu' 'san marcos' 'huehuetenango' 'quiche' 'baja verapaz'\n",
      " 'alta verapaz' 'peten' 'izabal' 'zacapa' 'chiquimula' 'jalapa' 'jutiapa']\n",
      "7 ['carreter' 'camino d' 'veredas' 'mar, lag' 'tren (vi' 'avión (v'\n",
      " 'otro, cu']\n"
     ]
    }
   ],
   "source": [
    "print(consumo5['depto'].nunique(), consumo5['depto'].unique())\n",
    "print(ecom02['c02act'].nunique(), ecom02['c02act'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac232c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create provincial number - name mapping\n",
    "province_map = {\n",
    "    1: 'guatemala', 2: 'el progreso', 3: 'sacatepequez', 4: 'chimaltenango',\n",
    "    5: 'escuintla', 6: 'santa rosa', 7: 'solola', 8: 'totonicapan',\n",
    "    9: 'quetzaltenango', 10: 'suchitepequez', 11: 'retalhuleu', 12: 'san marcos',\n",
    "    13: 'huehuetenango', 14: 'quiche', 15: 'baja verapaz', 16: 'alta verapaz',\n",
    "    17: 'peten', 18: 'izabal', 19: 'zacapa', 20: 'chiquimula',\n",
    "    21: 'jalapa', 22: 'jutiapa'\n",
    "}\n",
    "ecom02['depto_name'] = ecom02['depto'].map(province_map)\n",
    "\n",
    "# OrderedDict ensures order\n",
    "transport_translation = OrderedDict([\n",
    "    (0, 'paved_or_gravel_road'),\n",
    "    (1, 'dirt_road'),\n",
    "    (2, 'footpath_no_gravel'),\n",
    "    (3, 'footpath'),\n",
    "    (4, 'sea_lake_river'),\n",
    "    (5, 'train'),\n",
    "    (6, 'airplane'),\n",
    "    (7, 'other')\n",
    "])\n",
    "ecom02['transport_index'] = ecom02.groupby(['depto_name', 'mupio', 'sector']).cumcount()\n",
    "ecom02['transport_en'] = ecom02['transport_index'].map(transport_translation)\n",
    "\n",
    "# ）The presence or absence of standardized transportation facilities (si/no)\n",
    "ecom02['has_transport'] = ecom02['c02a01'].str.lower().str.strip()\n",
    "\n",
    "# pivot to wide format\n",
    "ecom02_wide = ecom02.pivot_table(\n",
    "    index=['depto_name', 'mupio', 'sector'],\n",
    "    columns='transport_en',\n",
    "    values='has_transport',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# rename to has_<type>\n",
    "ecom02_wide.columns = ['depto', 'mupio', 'sector'] + [f'has_{col}' for col in ecom02_wide.columns[3:]]\n",
    "\n",
    "# prepare consumo5\n",
    "consumo5['hhid'] = consumo5['hhid'].astype(pd.Int64Dtype()).astype(str)\n",
    "consumo5['depto'] = consumo5['depto'].str.strip().str.lower()\n",
    "consumo5['mupio'] = consumo5['mupio'].astype(int)\n",
    "consumo5['sector'] = consumo5['sector'].astype(int)\n",
    "\n",
    "# merge with transport data\n",
    "merged = consumo5.merge(\n",
    "    ecom02_wide,\n",
    "    on=['depto', 'mupio', 'sector'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# missing\n",
    "transport_cols = [col for col in merged.columns if col.startswith('has_')]\n",
    "merged[transport_cols] = merged[transport_cols].fillna('missing')\n",
    "\n",
    "# ensure order\n",
    "transport_cols_ordered = [f'has_{v}' for v in transport_translation.values()]\n",
    "final_cols = ['hhid'] + transport_cols_ordered\n",
    "merged = merged[final_cols]\n",
    "\n",
    "merged.to_csv('../cleaned_data/ECOM02_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# province_map（name → code）\n",
    "province_code_map = {v: k for k, v in province_map.items()}\n",
    "\n",
    "# SExtract community identification information\n",
    "community_info = consumo5[['hhid', 'depto', 'mupio', 'sector']].copy()\n",
    "community_info['depto_code'] = community_info['depto'].map(province_code_map)\n",
    "community_info['depto_name'] = community_info['depto']\n",
    "\n",
    "# reorder\n",
    "community_info = community_info[['hhid', 'depto_code', 'depto_name', 'mupio', 'sector']]\n",
    "\n",
    "community_info.to_csv('../cleaned_data/geo_info_per_household.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56ef63",
   "metadata": {},
   "source": [
    "## 15. Clean ```ECOM03``` - infrastructure availablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3808d03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c02b01      351\n",
      "c02b02a    1597\n",
      "c02b02b    1535\n",
      "dtype: int64\n",
      "   depto  mupio  sector c02b01 c02b02a   c02b02b\n",
      "0      1      5      50     SI       1  Kilometr\n",
      "1      1      5      50     SI       1  Kilometr\n",
      "2      1      5      50     SI       1  Kilometr\n",
      "3      1      5      50     SI       1  Kilometr\n",
      "4      1      5      50     SI       1  Kilometr\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ecom03 = pd.read_csv('../intermediate_data/ECOM03.csv', encoding='utf-8-sig')\n",
    "missing_report = ecom03.isnull().sum()\n",
    "print(missing_report[missing_report>0])\n",
    "print(ecom03.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1a3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "facility_dict = OrderedDict({\n",
    "    1: \"preschool\",\n",
    "    2: \"primary_school\",\n",
    "    3: \"secondary_school\",\n",
    "    4: \"health_center\",\n",
    "    5: \"public_hospital\",\n",
    "    6: \"private_hospital\",\n",
    "    7: \"private_clinic\",\n",
    "    8: \"natural_healer\",\n",
    "    9: \"traditional_midwife\",\n",
    "    10: \"pharmacy\",\n",
    "    11: \"public_phone\",\n",
    "    12: \"mail_service\",\n",
    "    13: \"bus_stop\",\n",
    "    14: \"bank\",\n",
    "    15: \"cooperative\",\n",
    "    16: \"police_post\",\n",
    "    17: \"civil_registry\",\n",
    "    18: \"market\",\n",
    "    19: \"church\",\n",
    "    20: \"community_room\",\n",
    "    21: \"recreation_parks\",\n",
    "    22: \"firehouse\",\n",
    "    23: \"firewood_collection\",\n",
    "    24: \"water_collection\",\n",
    "    25: \"work_site\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd75899",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_map = {\n",
    "    'metros': 1,\n",
    "    'kilometr': 1000,\n",
    "    'cuadras': 100,\n",
    "    'leguas': 5572,\n",
    "    'ns / nr': None,\n",
    "    None: None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d71d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "geo = pd.read_csv('../cleaned_data/geo_info_per_household.csv', encoding='utf-8-sig')\n",
    "\n",
    "ecom03['c02b02b'] = ecom03['c02b02b'].str.lower().str.strip()\n",
    "\n",
    "# Add the facility number (exactly 25 lines for each community)\n",
    "ecom03['infra_index'] = ecom03.groupby(['depto', 'mupio', 'sector']).cumcount() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e82e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecom03['unit_multiplier'] = ecom03['c02b02b'].map(unit_map)\n",
    "\n",
    "def compute_distance(row):\n",
    "    if pd.isna(row['c02b02a']) or pd.isna(row['unit_multiplier']):\n",
    "        return 99999999\n",
    "    try:\n",
    "        return float(row['c02b02a']) * row['unit_multiplier']\n",
    "    except:\n",
    "        return 99999999\n",
    "\n",
    "ecom03['distance_m'] = ecom03.apply(compute_distance, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff21e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the designated facilities\n",
    "subset_keys = list(facility_dict.keys())\n",
    "keep_keys = subset_keys[:15] + [18]\n",
    "ecom03 = ecom03[ecom03['infra_index'].isin(keep_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806510b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cols\n",
    "ecom03['facility'] = ecom03['infra_index'].map(facility_dict)\n",
    "ecom03['distance_col'] = 'distance_' + ecom03['facility']\n",
    "\n",
    "# Pivot to wide format\n",
    "distance_wide = ecom03.pivot_table(\n",
    "    index=['depto', 'mupio', 'sector'],\n",
    "    columns='distance_col',\n",
    "    values='distance_m',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0099b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize geo_info\n",
    "geo['depto'] = geo['depto_code'].astype(int)\n",
    "geo['mupio'] = geo['mupio'].astype(int)\n",
    "geo['sector'] = geo['sector'].astype(int)\n",
    "\n",
    "# Merge the distance of community infrastructure\n",
    "merged = geo.merge(distance_wide, on=['depto', 'mupio', 'sector'], how='left')\n",
    "\n",
    "# change missing to 99999999\n",
    "distance_cols = [f'distance_{facility_dict[k]}' for k in keep_keys]\n",
    "merged[distance_cols] = merged[distance_cols].fillna(99999999)\n",
    "\n",
    "ordered_cols = (\n",
    "    ['hhid'] +\n",
    "    distance_cols\n",
    ")\n",
    "merged = merged[ordered_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9061b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('../cleaned_data/ECOM03_cleaned.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eeb533",
   "metadata": {},
   "source": [
    "## 16. Merge all data based on ```hhid```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655bfd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking row counts...\n",
      "\n",
      "Checking hhid consistency...\n",
      "\n",
      "Consistency check complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 设置文件路径\n",
    "cleaned_path = '../cleaned_data'\n",
    "files = [f for f in os.listdir(cleaned_path) if f.endswith('.csv')]\n",
    "\n",
    "hhid_sets = {}\n",
    "row_counts = {}\n",
    "\n",
    "for file in files:\n",
    "    path = os.path.join(cleaned_path, file)\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to read {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if 'hhid' not in df.columns:\n",
    "        print(f\"[MISSING] 'hhid' column not found in {file}\")\n",
    "        continue\n",
    "\n",
    "    hhid_list = df['hhid'].astype(str).tolist()\n",
    "    hhid_sets[file] = set(hhid_list)\n",
    "    row_counts[file] = len(hhid_list)\n",
    "\n",
    "# 1. check rows num\n",
    "print(\"\\nChecking row counts...\")\n",
    "base_file, base_count = next(iter(row_counts.items()))\n",
    "for file, count in row_counts.items():\n",
    "    if count != base_count:\n",
    "        print(f\"[ROW COUNT MISMATCH] {file}: {count} rows (expected {base_count})\")\n",
    "\n",
    "# 2. check hhid\n",
    "print(\"\\nChecking hhid consistency...\")\n",
    "base_hhids = hhid_sets[base_file]\n",
    "for file, hhids in hhid_sets.items():\n",
    "    if hhids != base_hhids:\n",
    "        missing = base_hhids - hhids\n",
    "        extra = hhids - base_hhids\n",
    "        print(f\"[HHID MISMATCH] {file}:\")\n",
    "        if missing:\n",
    "            print(f\"  ↳ Missing hhids (vs base): {sorted(list(missing))[:5]}...\")\n",
    "        if extra:\n",
    "            print(f\"  ↳ Extra hhids (not in base): {sorted(list(extra))[:5]}...\")\n",
    "\n",
    "print(\"\\nConsistency check complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7404d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging: ECV17E14_cleaned.csv (39 columns)\n",
      "Merging: ECV01H01_cleaned.csv (29 columns)\n",
      "Merging: ECV31A16_cleaned.csv (12 columns)\n",
      "Merging: CONSUMO5_cleaned.csv (7 columns)\n",
      "Merging: ECV18N15_cleaned.csv (3 columns)\n",
      "Merging: ECV21A16_cleaned.csv (2 columns)\n",
      "Merging: ECOM03_cleaned.csv (17 columns)\n",
      "Merging: ECV02H01_cleaned.csv (29 columns)\n",
      "Merging: ECV28A16_cleaned.csv (27 columns)\n",
      "Merging: AUTOIDH_cleaned.csv (3 columns)\n",
      "Merging: ECV11P10_cleaned.csv (6 columns)\n",
      "Merging: ECV09P05_cleaned.csv (16 columns)\n",
      "Merging: ECOM02_cleaned.csv (9 columns)\n",
      "\n",
      "Final merged dataset saved to: ../cleaned_data/merged_all_data.csv\n",
      "Final shape: 7276 rows, 191 columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaned_path = '../cleaned_data'\n",
    "all_files = [f for f in os.listdir(cleaned_path) if f.endswith('.csv')]\n",
    "\n",
    "merge_files = [f for f in all_files if f != 'geo_info_per_household.csv']\n",
    "\n",
    "# main table: HOGARES_cleaned.csv\n",
    "merge_files_sorted = ['HOGARES_cleaned.csv'] + [f for f in merge_files if f != 'HOGARES_cleaned.csv']\n",
    "main_df = pd.read_csv(os.path.join(cleaned_path, merge_files_sorted[0]), encoding='utf-8-sig')\n",
    "\n",
    "# all left joint\n",
    "for file in merge_files_sorted[1:]:\n",
    "    path = os.path.join(cleaned_path, file)\n",
    "    df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "    print(f\"Merging: {file} ({df.shape[1]} columns)\")\n",
    "    main_df = main_df.merge(df, on='hhid', how='left')\n",
    "\n",
    "output_path = os.path.join(cleaned_path, 'merged_all_data.csv')\n",
    "main_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\nFinal merged dataset saved to: {output_path}\")\n",
    "print(f\"Final shape: {main_df.shape[0]} rows, {main_df.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa44ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in the merged dataset.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_path = '../cleaned_data/merged_all_data.csv'\n",
    "df = pd.read_csv(merged_path, encoding='utf-8-sig')\n",
    "\n",
    "total_missing = df.isnull().sum().sum()\n",
    "if total_missing == 0:\n",
    "    print(\"No missing values in the merged dataset.\")\n",
    "else:\n",
    "    print(f\"Total missing values in dataset: {total_missing}\")\n",
    "    print(\"\\n📌 Missing value count per column (non-zero only):\")\n",
    "    missing_per_column = df.isnull().sum()\n",
    "    print(missing_per_column[missing_per_column > 0].sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nTop 10 columns with most missing values:\")\n",
    "    print(missing_per_column.sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1afc3",
   "metadata": {},
   "source": [
    "## Generate ```country_name_metadata.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_data_type(series):\n",
    "    if pd.api.types.is_numeric_dtype(series):\n",
    "        return 'numeric'\n",
    "    else:\n",
    "        return 'categorical'\n",
    "\n",
    "variable_info = pd.DataFrame({\n",
    "    'variable_name': df.columns,\n",
    "    'data_type': [infer_data_type(df[col]) for col in df.columns]\n",
    "})\n",
    "\n",
    "variable_info.to_csv('../metadata/final_variable_types.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d946bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "var_types_df = pd.read_csv('../metadata/final_variable_types.csv', encoding='utf-8-sig')\n",
    "metadata_df = pd.read_csv('../metadata/guatemala_metadata.csv', encoding='utf-8-sig')\n",
    "\n",
    "var_types_df['variable_name'] = var_types_df['variable_name'].str.strip().str.lower()\n",
    "metadata_df['variable_name'] = metadata_df['variable_name'].str.strip().str.lower()\n",
    "\n",
    "merged_df = var_types_df.merge(metadata_df, on='variable_name', how='left')\n",
    "\n",
    "merged_df.to_csv('../metadata/final_variable_types.csv', index=False, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
